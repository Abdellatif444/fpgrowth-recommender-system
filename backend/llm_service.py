"""
Service d'int√©gration LLM via Groq API
"""
import os
import requests
from typing import List, Dict, Optional

class LLMService:
    def __init__(self):
        self.api_key = os.getenv('LLMGATEWAY_API_KEY', '')
        self.base_url = "https://api.groq.com/openai/v1"
        self.model = "llama-3.3-70b-versatile"  # Dernier mod√®le Llama 3.3
    
    def explain_recommendation(self, 
                              basket_items: List[str], 
                              recommended_item: str,
                              confidence: float,
                              lift: float) -> str:
        """
        G√©n√®re une explication en langage naturel pour une recommandation
        """
        prompt = f"""
        En tant qu'expert en e-commerce, explique pourquoi ce produit est recommand√© :
        
        Panier actuel : {', '.join(basket_items)}
        Produit recommand√© : {recommended_item}
        Confiance : {confidence:.1%}
        Force d'association (lift) : {lift:.2f}
        
        Fournis une explication courte (max 2 phrases) et engageante pour le client.
        Mets en avant la synergie entre les produits.
        """
        
        return self._call_llm(prompt)
    
    def detect_shopping_context(self, basket_items: List[str]) -> Dict:
        """
        D√©tecte le contexte d'achat (√©v√©nement, th√®me, besoin)
        """
        prompt = f"""
        Analyse ce panier d'achat et d√©termine :
        1. Le contexte probable (mariage, anniversaire, d√©coration maison, etc.)
        2. Le style d√©tect√© (moderne, vintage, √©l√©gant, etc.)
        3. 3 suggestions de produits compl√©mentaires
        
        Panier : {', '.join(basket_items)}
        
        R√©ponds au format JSON :
        {{
            "context": "...",
            "style": "...",
            "suggestions": ["...", "...", "..."],
            "reasoning": "..."
        }}
        """
        
        response = self._call_llm(prompt)
        try:
            import json
            return json.loads(response)
        except:
            return {
                "context": "Non d√©tect√©",
                "style": "Mixte",
                "suggestions": [],
                "reasoning": response
            }
    
    def generate_product_bundle_description(self, 
                                           items: List[str],
                                           bundle_name: str = None) -> str:
        """
        G√©n√®re une description marketing pour un bundle de produits
        """
        prompt = f"""
        Cr√©e une description marketing attractive pour ce bundle de produits :
        
        Produits : {', '.join(items)}
        Nom du bundle : {bundle_name or 'Pack Sp√©cial'}
        
        La description doit :
        - √ätre courte (max 3 phrases)
        - Mettre en avant la compl√©mentarit√© des produits
        - Donner envie d'acheter
        - √ätre en fran√ßais
        """
        
        return self._call_llm(prompt)
    
    def chatbot_response(self, 
                        user_message: str,
                        conversation_history: List[Dict] = None,
                        available_products: List[str] = None) -> str:
        """
        G√©n√®re une r√©ponse de chatbot pour aider le client
        """
        context = ""
        if available_products:
            context = f"\nüì¶ Produits populaires dans notre catalogue :\n" + "\n".join([f"- {p}" for p in available_products[:30]])
        
        history = ""
        if conversation_history:
            history = "\nüí¨ Conversation pr√©c√©dente :\n" + "\n".join([
                f"{msg['role']}: {msg['content']}" 
                for msg in conversation_history[-6:]  # Derniers 6 messages
            ])
        
        prompt = f"""
Tu es un assistant shopping intelligent pour un site e-commerce de d√©coration, et tu t'appelles Luna üåü

üé≠ PERSONNALIT√â (VARIE TON STYLE √Ä CHAQUE R√âPONSE) :
- Parfois professionnelle et √©l√©gante üíº
- Parfois amicale et chaleureuse üòä
- Parfois enthousiaste et dr√¥le üòÑ
- Parfois po√©tique et inspirante ‚ú®

üß† COMP√âTENCES SP√âCIALES :
1. **Reconnaissance partielle** : Si le client √©crit "PINK ON STICK", cherche dans le catalogue ci-dessous les produits contenant ces mots-cl√©s.
2. **Correction intelligente** : Comprends les fautes d'orthographe et les noms incomplets.
3. **Suggestions proactives** : Propose des produits compl√©mentaires UNIQUEMENT s'ils sont dans le catalogue.
4. **Empathie** : R√©ponds aux salutations avec chaleur.

‚ö†Ô∏è R√àGLE ABSOLUE - NE JAMAIS INVENTER :
- Tu NE PEUX PAS inventer de noms de produits
- Tu DOIS UNIQUEMENT sugg√©rer des produits qui sont EXACTEMENT list√©s ci-dessous
- Si tu ne trouves PAS de correspondance, dis-le honn√™tement et propose de chercher autrement

{context}
{history}

üí¨ Client : {user_message}

üìù INSTRUCTIONS :
- Si salutation ‚Üí R√©ponds chaleureusement + propose ton aide
- Si nom de produit PARTIEL ‚Üí Cherche UNIQUEMENT dans la liste ci-dessus et cite le nom EXACT
- Si AUCUN produit ne correspond ‚Üí Dis honn√™tement "Je n'ai pas trouv√© de produit correspondant exactement" et propose de reformuler
- Si question produit ‚Üí Infos utiles + suggestions (UNIQUEMENT des produits de la liste)
- VARIE ton style √† chaque r√©ponse !
- Sois CONVAINCANTE mais HONN√äTE
- Maximum 3-4 phrases
- Utilise des emojis avec parcimonie (1-2 max)

Luna, r√©ponds maintenant (RAPPEL: N'invente JAMAIS de noms de produits !) :
        """
        
        return self._call_llm(prompt)
    
    def _call_llm(self, prompt: str, max_tokens: int = 250) -> str:
        """
        Appelle l'API Groq
        """
        if not self.api_key:
            return "LLM non configur√© (API key manquante)"
        
        try:
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": [
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "max_tokens": max_tokens,
                    "temperature": 0.8  # Plus de cr√©ativit√© pour Luna
                },
                timeout=15
            )
            
            if response.status_code == 200:
                data = response.json()
                return data['choices'][0]['message']['content'].strip()
            else:
                return f"Erreur LLM : {response.status_code}"
                
        except Exception as e:
            return f"Erreur lors de l'appel au LLM : {str(e)}"

# Instance globale
llm_service = LLMService()
