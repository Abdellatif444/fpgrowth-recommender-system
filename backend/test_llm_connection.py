"""
Script de test pour v√©rifier la connexion LLM Gateway
"""
import os
import sys
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

# Importer le service LLM
sys.path.append(os.path.dirname(__file__))
from llm_service import llm_service

def test_llm_connection():
    """Test de connexion LLM Gateway"""
    print("=" * 60)
    print("üß™ TEST DE CONNEXION LLM GATEWAY")
    print("=" * 60)
    
    # V√©rifier que la cl√© API est configur√©e
    api_key = os.getenv('LLMGATEWAY_API_KEY')
    if not api_key:
        print("‚ùå ERREUR: Cl√© API non trouv√©e dans .env")
        return False
    
    print(f"‚úÖ Cl√© API trouv√©e: {api_key[:20]}...")
    
    # Test 1: Explication simple
    print("\nüìù Test 1: Explication de recommandation")
    print("-" * 60)
    
    explanation = llm_service.explain_recommendation(
        basket_items=["WHITE HANGING HEART T-LIGHT HOLDER"],
        recommended_item="REGENCY CAKESTAND 3 TIER",
        confidence=0.65,
        lift=3.2
    )
    
    print(f"R√©sultat: {explanation}")
    
    if "Erreur" in explanation or "LLM non configur√©" in explanation:
        print("‚ùå Test √©chou√©")
        return False
    else:
        print("‚úÖ Test r√©ussi")
    
    # Test 2: D√©tection de contexte
    print("\nüîç Test 2: D√©tection de contexte")
    print("-" * 60)
    
    context = llm_service.detect_shopping_context([
        "WHITE HANGING HEART T-LIGHT HOLDER",
        "PARTY BUNTING",
        "REGENCY CAKESTAND 3 TIER"
    ])
    
    print(f"Contexte d√©tect√©: {context.get('context', 'N/A')}")
    print(f"Style: {context.get('style', 'N/A')}")
    print(f"Suggestions: {', '.join(context.get('suggestions', []))}")
    
    if context.get('context') == "Non d√©tect√©":
        print("‚ùå Test √©chou√©")
        return False
    else:
        print("‚úÖ Test r√©ussi")
    
    # Test 3: Chatbot
    print("\nüí¨ Test 3: Chatbot")
    print("-" * 60)
    
    response = llm_service.chatbot_response(
        user_message="Bonjour, je cherche des d√©corations pour une f√™te",
        available_products=["WHITE HANGING HEART", "PARTY BUNTING", "REGENCY CAKESTAND"]
    )
    
    print(f"R√©ponse: {response}")
    
    if "Erreur" in response:
        print("‚ùå Test √©chou√©")
        return False
    else:
        print("‚úÖ Test r√©ussi")
    
    # R√©sum√© final
    print("\n" + "=" * 60)
    print("üéâ TOUS LES TESTS SONT PASS√âS !")
    print("=" * 60)
    print("\n‚úÖ L'int√©gration LLM Gateway est fonctionnelle")
    print("‚úÖ Vous pouvez maintenant utiliser les fonctionnalit√©s IA")
    print("\nProchaines √©tapes:")
    print("  1. Red√©marrer le backend: docker-compose restart backend")
    print("  2. Int√©grer les endpoints dans app.py")
    print("  3. Ajouter l'interface frontend")
    
    return True

if __name__ == "__main__":
    try:
        success = test_llm_connection()
        sys.exit(0 if success else 1)
    except Exception as e:
        print(f"\n‚ùå ERREUR FATALE: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
